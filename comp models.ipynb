{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1968a398",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Models comparsion\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a2ab29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed7b14b",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Custom Neural Network\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32fb6fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] | Loss: 0.4667 | Acc: 84.90%\n",
      "Epoch [2/10] | Loss: 0.1599 | Acc: 95.13%\n",
      "Epoch [3/10] | Loss: 0.1153 | Acc: 96.48%\n",
      "Epoch [4/10] | Loss: 0.0967 | Acc: 97.05%\n",
      "Epoch [5/10] | Loss: 0.0809 | Acc: 97.44%\n",
      "Epoch [6/10] | Loss: 0.0657 | Acc: 97.93%\n",
      "Epoch [7/10] | Loss: 0.0556 | Acc: 98.27%\n",
      "Epoch [8/10] | Loss: 0.0498 | Acc: 98.38%\n",
      "Epoch [9/10] | Loss: 0.0456 | Acc: 98.59%\n",
      "Epoch [10/10] | Loss: 0.0367 | Acc: 98.81%\n",
      "\n",
      "ğŸ•’ Total Training Time: 93.27 seconds\n",
      "\n",
      "ğŸ¯ Final Test Accuracy: 97.19%\n"
     ]
    }
   ],
   "source": [
    "best_lr = 0.01\n",
    "best_batch_size = 32\n",
    "best_arch = [512, 256, 128, 64]\n",
    "\n",
    "\n",
    "\n",
    "# ======  Best Model Architecture ======\n",
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, layer_sizes, activation='relu'):\n",
    "    \n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            self.activation = F.relu\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = torch.tanh\n",
    "        elif activation == 'leaky_relu':\n",
    "            self.activation = F.leaky_relu\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.activation(layer(x))\n",
    "        x = self.layers[-1](x)  \n",
    "        return x\n",
    "    \n",
    "model = FeedforwardNN([784] + best_arch + [10], activation='relu')\n",
    "\n",
    "# ====== 2. Prepare Combined Train + Validation Data ======\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "full_train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "\n",
    "train_size = int(0.83 * len(full_train_data))   # ~50,000\n",
    "val_size = len(full_train_data) - train_size    # ~10,000\n",
    "train_data, val_data = random_split(full_train_data, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "\n",
    "test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "combined_dataset = ConcatDataset([train_data, val_data])\n",
    "combined_loader = DataLoader(combined_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# ====== 3. Train Final Model ======\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "start_time = time.time()    #start time \n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    for images, labels in combined_loader:\n",
    "        images = images.view(images.size(0), -1)  # flatten 28x28 -> 784\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] | Loss: {total_loss/len(combined_loader):.4f} | \"\n",
    "    f\"Acc: {100*correct/total:.2f}%\")\n",
    "    \n",
    "    \n",
    "end_time = time.time()   # end time\n",
    "training_time_nn = end_time - start_time\n",
    "print(f\"\\nğŸ•’ Total Training Time: {training_time_nn:.2f} seconds\")\n",
    "# ====== 5. Evaluate on Test Set ======\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "test_loss_nn = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(images.size(0), -1)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)  \n",
    "        test_loss_nn += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred.extend(preds.numpy())\n",
    "\n",
    "y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "loss__nn=nn.CrossEntropyLoss()(outputs, labels).item();\n",
    "test_acc = np.mean(y_true == y_pred)\n",
    "print(f\"\\nğŸ¯ Final Test Accuracy: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b452137f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziademad/Assigment_2/.venv/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-11-02 20:21:07.595352: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 150528000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m6000/6000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 5ms/step - accuracy: 0.9194 - loss: 0.2889 - val_accuracy: 0.9716 - val_loss: 0.0989\n",
      "Epoch 2/10\n",
      "\u001b[1m6000/6000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 4ms/step - accuracy: 0.9606 - loss: 0.1282 - val_accuracy: 0.9663 - val_loss: 0.1051\n",
      "Epoch 3/10\n",
      "\u001b[1m6000/6000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 4ms/step - accuracy: 0.9674 - loss: 0.1050 - val_accuracy: 0.9810 - val_loss: 0.0673\n",
      "Epoch 4/10\n",
      "\u001b[1m6000/6000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 4ms/step - accuracy: 0.9708 - loss: 0.0945 - val_accuracy: 0.9822 - val_loss: 0.0633\n",
      "Epoch 5/10\n",
      "\u001b[1m6000/6000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 4ms/step - accuracy: 0.9735 - loss: 0.0863 - val_accuracy: 0.9787 - val_loss: 0.0728\n",
      "Epoch 6/10\n",
      "\u001b[1m6000/6000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 4ms/step - accuracy: 0.9736 - loss: 0.0842 - val_accuracy: 0.9843 - val_loss: 0.0574\n",
      "Epoch 7/10\n",
      "\u001b[1m6000/6000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 4ms/step - accuracy: 0.9748 - loss: 0.0799 - val_accuracy: 0.9833 - val_loss: 0.0590\n",
      "Epoch 8/10\n",
      "\u001b[1m6000/6000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 4ms/step - accuracy: 0.9754 - loss: 0.0763 - val_accuracy: 0.9816 - val_loss: 0.0582\n",
      "Epoch 9/10\n",
      "\u001b[1m6000/6000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 4ms/step - accuracy: 0.9769 - loss: 0.0745 - val_accuracy: 0.9822 - val_loss: 0.0584\n",
      "Epoch 10/10\n",
      "\u001b[1m6000/6000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 4ms/step - accuracy: 0.9775 - loss: 0.0699 - val_accuracy: 0.9841 - val_loss: 0.0576\n",
      "Best CNN Test Accuracy: 0.9828\n",
      "Best CNN Test Loss: 0.0507\n",
      "Best CNN Test Time: 1.36 seconds\n",
      "\n",
      "ğŸ•’ Total Training Time: 266.46 seconds\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "train_images = np.expand_dims(train_images, axis=-1) # add channel dimension\n",
    "test_images = np.expand_dims(test_images, axis=-1)\n",
    "\n",
    "best_model = Sequential()\n",
    "best_model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='leaky_relu', input_shape=(28, 28, 1)))\n",
    "best_model.add(BatchNormalization())\n",
    "best_model.add(Dropout(0.5))\n",
    "best_model.add(Conv2D(filters=8, kernel_size=(3, 3), activation='leaky_relu'))\n",
    "best_model.add(BatchNormalization())\n",
    "best_model.add(Dropout(0.5))\n",
    "best_model.add(Flatten()) # flatten the 2D arrays to 1D\n",
    "best_model.add(Dense(10, activation='softmax'))\n",
    "best_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "callbacks = [\n",
    "    ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss'),\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "]\n",
    "start_time = time.time()    #start time \n",
    "history = best_model.fit(train_images, train_labels, epochs=10, batch_size=8, validation_split=0.2, callbacks=callbacks)\n",
    "end_time = time.time()   # end time\n",
    "training_time_cnn = end_time - start_time\n",
    "\n",
    "start_test_time = time.time()\n",
    "cnn_test_accuracy = best_model.evaluate(test_images, test_labels, verbose=0)[1]\n",
    "cnn_loss = best_model.evaluate(test_images, test_labels, verbose=0)[0]\n",
    "end_test_time = time.time()\n",
    "test_time = end_test_time - start_test_time\n",
    "\n",
    "print(f'Best CNN Test Accuracy: {cnn_test_accuracy:.4f}')\n",
    "print(f'Best CNN Test Loss: {cnn_loss:.4f}')\n",
    "print(f'Best CNN Test Time: {test_time:.2f} seconds')\n",
    "print(f\"\\nğŸ•’ Total Training Time: {training_time_cnn:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ddf76c",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# model comparison table\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ccdcc345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Test Accuracy (%)</th>\n",
       "      <th>Test Loss</th>\n",
       "      <th>Training Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression (Manual)</td>\n",
       "      <td>99.763191</td>\n",
       "      <td>0.012224</td>\n",
       "      <td>0.922057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Softmax Regression (Manual)</td>\n",
       "      <td>90.764284</td>\n",
       "      <td>0.334960</td>\n",
       "      <td>8.376068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Softmax Regression (Built-in)</td>\n",
       "      <td>90.764284</td>\n",
       "      <td>0.334960</td>\n",
       "      <td>0.000791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Neural Network (Best Params)</td>\n",
       "      <td>97.190000</td>\n",
       "      <td>0.002892</td>\n",
       "      <td>93.265222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNN (Best Model)</td>\n",
       "      <td>98.280001</td>\n",
       "      <td>0.050707</td>\n",
       "      <td>266.463160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Model  Test Accuracy (%)  Test Loss  \\\n",
       "0   Logistic Regression (Manual)          99.763191   0.012224   \n",
       "1    Softmax Regression (Manual)          90.764284   0.334960   \n",
       "2  Softmax Regression (Built-in)          90.764284   0.334960   \n",
       "3   Neural Network (Best Params)          97.190000   0.002892   \n",
       "4               CNN (Best Model)          98.280001   0.050707   \n",
       "\n",
       "   Training Time (s)  \n",
       "0           0.922057  \n",
       "1           8.376068  \n",
       "2           0.000791  \n",
       "3          93.265222  \n",
       "4         266.463160  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"logreg_results.json\", \"r\") as f:\n",
    "    lr_results = json.load(f)\n",
    "\n",
    "with open(\"softmax_results.json\", \"r\") as f:\n",
    "    softmax_results = json.load(f)\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame([\n",
    "    {'Model': 'Logistic Regression (Manual)', 'Test Accuracy (%)': lr_results[\"test_accuracy\"] * 100, 'Test Loss': lr_results[\"test_loss\"], 'Training Time (s)': lr_results[\"training_time_lr\"]},\n",
    "    {'Model': 'Softmax Regression (Manual)', 'Test Accuracy (%)': softmax_results[\"test_accuracy_manual\"] * 100, 'Test Loss': softmax_results[\"test_loss_manual\"], 'Training Time (s)': softmax_results[\"train_time_manual\"]},\n",
    "    {'Model': 'Softmax Regression (Built-in)', 'Test Accuracy (%)': softmax_results[\"test_accuracy_builtin\"] * 100, 'Test Loss': softmax_results[\"test_loss_builtin\"], 'Training Time (s)': softmax_results[\"training_time_builtin\"]},\n",
    "    {'Model': 'Neural Network (Best Params)', 'Test Accuracy (%)': test_acc * 100, 'Test Loss': loss__nn, 'Training Time (s)': training_time_nn},\n",
    "    {'Model': 'CNN (Best Model)', 'Test Accuracy (%)': cnn_test_accuracy * 100, 'Test Loss': cnn_loss, 'Training Time (s)': training_time_cnn}\n",
    "])\n",
    "\n",
    "display(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
