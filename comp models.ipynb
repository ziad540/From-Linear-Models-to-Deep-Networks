{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1968a398",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Models comparsion\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a2ab29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 01:29:36.282826: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed7b14b",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Custom Neural Network\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32fb6fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] | Loss: 0.4667 | Acc: 84.90%\n",
      "Epoch [2/10] | Loss: 0.1599 | Acc: 95.13%\n",
      "Epoch [3/10] | Loss: 0.1153 | Acc: 96.48%\n",
      "Epoch [4/10] | Loss: 0.0967 | Acc: 97.05%\n",
      "Epoch [5/10] | Loss: 0.0809 | Acc: 97.44%\n",
      "Epoch [6/10] | Loss: 0.0657 | Acc: 97.93%\n",
      "Epoch [7/10] | Loss: 0.0556 | Acc: 98.27%\n",
      "Epoch [8/10] | Loss: 0.0498 | Acc: 98.38%\n",
      "Epoch [9/10] | Loss: 0.0456 | Acc: 98.59%\n",
      "Epoch [10/10] | Loss: 0.0367 | Acc: 98.81%\n",
      "\n",
      "ðŸ•’ Total Training Time: 93.27 seconds\n",
      "\n",
      "ðŸŽ¯ Final Test Accuracy: 97.19%\n"
     ]
    }
   ],
   "source": [
    "best_lr = 0.01\n",
    "best_batch_size = 32\n",
    "best_arch = [512, 256, 128, 64]\n",
    "\n",
    "\n",
    "\n",
    "# ======  Best Model Architecture ======\n",
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, layer_sizes, activation='relu'):\n",
    "    \n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            self.activation = F.relu\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = torch.tanh\n",
    "        elif activation == 'leaky_relu':\n",
    "            self.activation = F.leaky_relu\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.activation(layer(x))\n",
    "        x = self.layers[-1](x)  \n",
    "        return x\n",
    "    \n",
    "model = FeedforwardNN([784] + best_arch + [10], activation='relu')\n",
    "\n",
    "# ====== 2. Prepare Combined Train + Validation Data ======\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "full_train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "\n",
    "train_size = int(0.83 * len(full_train_data))   # ~50,000\n",
    "val_size = len(full_train_data) - train_size    # ~10,000\n",
    "train_data, val_data = random_split(full_train_data, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "\n",
    "test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "combined_dataset = ConcatDataset([train_data, val_data])\n",
    "combined_loader = DataLoader(combined_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# ====== 3. Train Final Model ======\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "start_time = time.time()    #start time \n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    for images, labels in combined_loader:\n",
    "        images = images.view(images.size(0), -1)  # flatten 28x28 -> 784\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] | Loss: {total_loss/len(combined_loader):.4f} | \"\n",
    "    f\"Acc: {100*correct/total:.2f}%\")\n",
    "    \n",
    "    \n",
    "end_time = time.time()   # end time\n",
    "training_time_nn = end_time - start_time\n",
    "print(f\"\\nðŸ•’ Total Training Time: {training_time_nn:.2f} seconds\")\n",
    "# ====== 5. Evaluate on Test Set ======\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "test_loss_nn = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(images.size(0), -1)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)  \n",
    "        test_loss_nn += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred.extend(preds.numpy())\n",
    "\n",
    "y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "loss__nn=nn.CrossEntropyLoss()(outputs, labels).item();\n",
    "test_acc = np.mean(y_true == y_pred)\n",
    "print(f\"\\nðŸŽ¯ Final Test Accuracy: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b452137f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziademad/Assigment_2/.venv/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-11-02 20:21:07.595352: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 150528000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m6000/6000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 5ms/step - accuracy: 0.9194 - loss: 0.2889 - val_accuracy: 0.9716 - val_loss: 0.0989\n",
      "Epoch 2/10\n",
      "\u001b[1m6000/6000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 4ms/step - accuracy: 0.9606 - loss: 0.1282 - val_accuracy: 0.9663 - val_loss: 0.1051\n",
      "Epoch 3/10\n",
      "\u001b[1m6000/6000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 4ms/step - accuracy: 0.9674 - loss: 0.1050 - val_accuracy: 0.9810 - val_loss: 0.0673\n",
      "Epoch 4/10\n",
      "\u001b[1m6000/6000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 4ms/step - accuracy: 0.9708 - loss: 0.0945 - val_accuracy: 0.9822 - val_loss: 0.0633\n",
      "Epoch 5/10\n",
      "\u001b[1m6000/6000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 4ms/step - accuracy: 0.9735 - loss: 0.0863 - val_accuracy: 0.9787 - val_loss: 0.0728\n",
      "Epoch 6/10\n",
      "\u001b[1m6000/6000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 4ms/step - accuracy: 0.9736 - loss: 0.0842 - val_accuracy: 0.9843 - val_loss: 0.0574\n",
      "Epoch 7/10\n",
      "\u001b[1m6000/6000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 4ms/step - accuracy: 0.9748 - loss: 0.0799 - val_accuracy: 0.9833 - val_loss: 0.0590\n",
      "Epoch 8/10\n",
      "\u001b[1m6000/6000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 4ms/step - accuracy: 0.9754 - loss: 0.0763 - val_accuracy: 0.9816 - val_loss: 0.0582\n",
      "Epoch 9/10\n",
      "\u001b[1m6000/6000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 4ms/step - accuracy: 0.9769 - loss: 0.0745 - val_accuracy: 0.9822 - val_loss: 0.0584\n",
      "Epoch 10/10\n",
      "\u001b[1m6000/6000\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 4ms/step - accuracy: 0.9775 - loss: 0.0699 - val_accuracy: 0.9841 - val_loss: 0.0576\n",
      "Best CNN Test Accuracy: 0.9828\n",
      "Best CNN Test Loss: 0.0507\n",
      "Best CNN Test Time: 1.36 seconds\n",
      "\n",
      "ðŸ•’ Total Training Time: 266.46 seconds\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "train_images = np.expand_dims(train_images, axis=-1) # add channel dimension\n",
    "test_images = np.expand_dims(test_images, axis=-1)\n",
    "\n",
    "best_model = Sequential()\n",
    "best_model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='leaky_relu', input_shape=(28, 28, 1)))\n",
    "best_model.add(BatchNormalization())\n",
    "best_model.add(Dropout(0.5))\n",
    "best_model.add(Conv2D(filters=8, kernel_size=(3, 3), activation='leaky_relu'))\n",
    "best_model.add(BatchNormalization())\n",
    "best_model.add(Dropout(0.5))\n",
    "best_model.add(Flatten()) # flatten the 2D arrays to 1D\n",
    "best_model.add(Dense(10, activation='softmax'))\n",
    "best_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "callbacks = [\n",
    "    ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss'),\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "]\n",
    "start_time = time.time()    #start time \n",
    "history = best_model.fit(train_images, train_labels, epochs=10, batch_size=8, validation_split=0.2, callbacks=callbacks)\n",
    "end_time = time.time()   # end time\n",
    "training_time_cnn = end_time - start_time\n",
    "\n",
    "start_test_time = time.time()\n",
    "cnn_test_accuracy = best_model.evaluate(test_images, test_labels, verbose=0)[1]\n",
    "cnn_loss = best_model.evaluate(test_images, test_labels, verbose=0)[0]\n",
    "end_test_time = time.time()\n",
    "test_time = end_test_time - start_test_time\n",
    "\n",
    "print(f'Best CNN Test Accuracy: {cnn_test_accuracy:.4f}')\n",
    "print(f'Best CNN Test Loss: {cnn_loss:.4f}')\n",
    "print(f'Best CNN Test Time: {test_time:.2f} seconds')\n",
    "print(f\"\\nðŸ•’ Total Training Time: {training_time_cnn:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ddf76c",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# model comparison table\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccdcc345",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33msoftmax_results.json\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      5\u001b[39m     softmax_results = json.load(f)\n\u001b[32m      8\u001b[39m df_results = pd.DataFrame([\n\u001b[32m      9\u001b[39m     {\u001b[33m'\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mLogistic Regression (Manual)\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTest Accuracy (\u001b[39m\u001b[33m%\u001b[39m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m: lr_results[\u001b[33m\"\u001b[39m\u001b[33mtest_accuracy\u001b[39m\u001b[33m\"\u001b[39m] * \u001b[32m100\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTest Loss\u001b[39m\u001b[33m'\u001b[39m: lr_results[\u001b[33m\"\u001b[39m\u001b[33mtest_loss\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m'\u001b[39m\u001b[33mTraining Time (s)\u001b[39m\u001b[33m'\u001b[39m: lr_results[\u001b[33m\"\u001b[39m\u001b[33mtraining_time_lr\u001b[39m\u001b[33m\"\u001b[39m]},\n\u001b[32m     10\u001b[39m     {\u001b[33m'\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mSoftmax Regression (Manual)\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTest Accuracy (\u001b[39m\u001b[33m%\u001b[39m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m: softmax_results[\u001b[33m\"\u001b[39m\u001b[33mtest_accuracy_manual\u001b[39m\u001b[33m\"\u001b[39m] * \u001b[32m100\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTest Loss\u001b[39m\u001b[33m'\u001b[39m: softmax_results[\u001b[33m\"\u001b[39m\u001b[33mtest_loss_manual\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m'\u001b[39m\u001b[33mTraining Time (s)\u001b[39m\u001b[33m'\u001b[39m: softmax_results[\u001b[33m\"\u001b[39m\u001b[33mtrain_time_manual\u001b[39m\u001b[33m\"\u001b[39m]},\n\u001b[32m     11\u001b[39m     {\u001b[33m'\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mSoftmax Regression (Built-in)\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTest Accuracy (\u001b[39m\u001b[33m%\u001b[39m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m: softmax_results[\u001b[33m\"\u001b[39m\u001b[33mtest_accuracy_builtin\u001b[39m\u001b[33m\"\u001b[39m] * \u001b[32m100\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTest Loss\u001b[39m\u001b[33m'\u001b[39m: softmax_results[\u001b[33m\"\u001b[39m\u001b[33mtest_loss_builtin\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m'\u001b[39m\u001b[33mTraining Time (s)\u001b[39m\u001b[33m'\u001b[39m: softmax_results[\u001b[33m\"\u001b[39m\u001b[33mtraining_time_builtin\u001b[39m\u001b[33m\"\u001b[39m]},\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     {\u001b[33m'\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mNeural Network (Best Params)\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTest Accuracy (\u001b[39m\u001b[33m%\u001b[39m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mtest_acc\u001b[49m * \u001b[32m100\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTest Loss\u001b[39m\u001b[33m'\u001b[39m: loss__nn, \u001b[33m'\u001b[39m\u001b[33mTraining Time (s)\u001b[39m\u001b[33m'\u001b[39m: training_time_nn},\n\u001b[32m     13\u001b[39m     {\u001b[33m'\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mCNN (Best Model)\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTest Accuracy (\u001b[39m\u001b[33m%\u001b[39m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m: cnn_test_accuracy * \u001b[32m100\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTest Loss\u001b[39m\u001b[33m'\u001b[39m: cnn_loss, \u001b[33m'\u001b[39m\u001b[33mTraining Time (s)\u001b[39m\u001b[33m'\u001b[39m: training_time_cnn}\n\u001b[32m     14\u001b[39m ])\n\u001b[32m     16\u001b[39m display(df_results)\n",
      "\u001b[31mNameError\u001b[39m: name 'test_acc' is not defined"
     ]
    }
   ],
   "source": [
    "with open(\"logreg_results.json\", \"r\") as f:\n",
    "    lr_results = json.load(f)\n",
    "\n",
    "with open(\"softmax_results.json\", \"r\") as f:\n",
    "    softmax_results = json.load(f)\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame([\n",
    "    {'Model': 'Logistic Regression (Manual)', 'Test Accuracy (%)': lr_results[\"test_accuracy\"] * 100, 'Test Loss': lr_results[\"test_loss\"], 'Training Time (s)': lr_results[\"training_time_lr\"]},\n",
    "    {'Model': 'Softmax Regression (Manual)', 'Test Accuracy (%)': softmax_results[\"test_accuracy_manual\"] * 100, 'Test Loss': softmax_results[\"test_loss_manual\"], 'Training Time (s)': softmax_results[\"train_time_manual\"]},\n",
    "    {'Model': 'Softmax Regression (Built-in)', 'Test Accuracy (%)': softmax_results[\"test_accuracy_builtin\"] * 100, 'Test Loss': softmax_results[\"test_loss_builtin\"], 'Training Time (s)': softmax_results[\"training_time_builtin\"]},\n",
    "    {'Model': 'Neural Network (Best Params)', 'Test Accuracy (%)': test_acc * 100, 'Test Loss': loss__nn, 'Training Time (s)': training_time_nn},\n",
    "    {'Model': 'CNN (Best Model)', 'Test Accuracy (%)': cnn_test_accuracy * 100, 'Test Loss': cnn_loss, 'Training Time (s)': training_time_cnn}\n",
    "])\n",
    "\n",
    "display(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1480ed9",
   "metadata": {},
   "source": [
    "### ðŸš€ Model Comparison Summary (Accuracy vs Training Time)\n",
    "\n",
    "| Model | Accuracy | Training Time |\n",
    "|------|---------|---------------|\n",
    "| Logistic Regression (Manual) | **99.76%** | **0.92s** |\n",
    "| Softmax Regression (Built-in) | 90.76% | **0.0008s** |\n",
    "| Neural Network (Best Params) | 97.19% | 93.26s |\n",
    "| CNN (Best Model) | 98.28% | 266.46s |\n",
    "\n",
    "#### ðŸ“Œ Key Takeaways\n",
    "- **Logistic Regression** achieved the highest accuracy with extremely fast training.\n",
    "- **Built-in Softmax** is ideal for instant baseline performance checks.\n",
    "- **Neural Network** improves learning for non-linear patterns, but training is slow.\n",
    "- **CNN** excels in spatial feature extraction (best for images) but has the highest compute cost.\n",
    "\n",
    "#### ðŸ§  Usage Guide\n",
    "| When | Best Model |\n",
    "|------|-----------|\n",
    "Simple / linearly separable data | Logistic Regression |\n",
    "Fast prototyping | Built-in Softmax |\n",
    "Non-linear complex patterns | Neural Network |\n",
    "Image / spatial structure | CNN |\n",
    "\n",
    "> âœ… Simple models = fast + accurate for simple patterns  \n",
    "> âœ… Deep models = needed only when spatial or deeper feature learning matters  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
